{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_LDA_Topic_Modelling_Online_Review.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravsingla/NLP/blob/master/Text_LDA_Topic_Modelling_Online_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5d5lJTeFbvH",
        "outputId": "7a9c12e5-cebc-4c9d-c51c-2b5ddfacef3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "nltk.download('stopwords') # run this one time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orQCxuTKLlUn",
        "outputId": "dedecd03-c5e1-4219-b807-63d226f925a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.6MB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.32.3)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.22.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.13.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.6.9)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading https://files.pythonhosted.org/packages/47/a4/204fa23012e913839c2da4514b92f17da82bf5fc8c2c3d902fa3fa3c6eec/funcy-1.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (40.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (18.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (5.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.11.0)\n",
            "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.8.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.11 pyLDAvis-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQYUU7ybFvAm"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# libraries for visualization\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiyarK9QL1mG"
      },
      "source": [
        "df = pd.read_json('Automotive_5.json', lines=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4WJ1uMtL7I2"
      },
      "source": [
        "# function to plot most frequent terms\n",
        "def freq_words(x, terms = 30):\n",
        "  all_words = ' '.join([text for text in x])\n",
        "  all_words = all_words.split()\n",
        "\n",
        "  fdist = FreqDist(all_words)\n",
        "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
        "\n",
        "  # selecting top 20 most frequent words\n",
        "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
        "  plt.figure(figsize=(20,5))\n",
        "  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
        "  ax.set(ylabel = 'Count')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz7xefJhMY8Y"
      },
      "source": [
        "freq_words(df['reviewText'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TQ7LomgMcCl"
      },
      "source": [
        "# remove unwanted characters, numbers and symbols\n",
        "df['reviewText'] = df['reviewText'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-76E5UwMe2V"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZahjOYEMfv2"
      },
      "source": [
        "# function to remove stopwords\n",
        "def remove_stopwords(rev):\n",
        "    rev_new = \" \".join([i for i in rev if i not in stop_words])\n",
        "    return rev_new\n",
        "\n",
        "# remove short words (length < 3)\n",
        "df['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "\n",
        "# remove stopwords from the text\n",
        "reviews = [remove_stopwords(r.split()) for r in df['reviewText']]\n",
        "\n",
        "# make entire text lowercase\n",
        "reviews = [r.lower() for r in reviews]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg9y1NynMjVH"
      },
      "source": [
        "freq_words(reviews, 35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcjRg1-6MoHw"
      },
      "source": [
        "!python -m spacy download en # one time run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uoFYFY6MpUn"
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def lemmatization(texts, tags=['NOUN', 'ADJ']): # filter noun and adjective\n",
        "       output = []\n",
        "       for sent in texts:\n",
        "             doc = nlp(\" \".join(sent)) \n",
        "             output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
        "       return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeFli3SDM5wm"
      },
      "source": [
        "#Let’s tokenize the reviews and then lemmatize the tokenized reviews.\n",
        "tokenized_reviews = pd.Series(reviews).apply(lambda x: x.split())\n",
        "print(tokenized_reviews[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLOkctTENImP"
      },
      "source": [
        "#Let’s tokenize the reviews and then lemmatize the tokenized reviews.\n",
        "reviews_2 = lemmatization(tokenized_reviews)\n",
        "print(reviews_2[1]) # print lemmatized review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKDvS8LNSR2"
      },
      "source": [
        "#we have not just lemmatized the words but also filtered only nouns and adjectives\n",
        "reviews_3 = []\n",
        "for i in range(len(reviews_2)):\n",
        "    reviews_3.append(' '.join(reviews_2[i]))\n",
        "\n",
        "df['reviews'] = reviews_3\n",
        "\n",
        "freq_words(df['reviews'], 35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_EAxfIpN-Ov"
      },
      "source": [
        "dictionary = corpora.Dictionary(reviews_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4XkppPFN_hN"
      },
      "source": [
        "doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2qtCLsyOLnB"
      },
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "LDA = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=7, random_state=100,\n",
        "                chunksize=1000, passes=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YUE0DWPOXLg"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU9xGaClOcjK"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}