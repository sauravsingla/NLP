{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TextParsing_NLTK_SpaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravsingla/NLP/blob/master/NLP_TextParsing_NLTK_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQshbRMjj61M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cfdb8a2c-e5c1-406a-adc1-b871b9f36c7b"
      },
      "source": [
        "# Import nltk and download brown corpus\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd-eAgNUj-Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load brown corpus \n",
        "# created in 1961 and organized in files with text from 500 sources\n",
        "\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlChUlGwmqbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "d818ed16-394c-4eda-c027-f20dbd1ebd31"
      },
      "source": [
        "dir(brown)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_LazyCorpusLoader__args',\n",
              " '_LazyCorpusLoader__kwargs',\n",
              " '_LazyCorpusLoader__load',\n",
              " '_LazyCorpusLoader__name',\n",
              " '_LazyCorpusLoader__reader_cls',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " '__weakref__',\n",
              " '_unload',\n",
              " 'subdir',\n",
              " 'unicode_repr']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX5rrLD8kEOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f579e7ca-b5a4-4e96-b5fb-6e9839c4c581"
      },
      "source": [
        "brown.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FKFDNLokFHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6236b178-f61c-4fae-e645-5761870bd993"
      },
      "source": [
        "len(brown.words())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohvcwZdkG5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8ca2bdca-bdad-4e80-bbe5-604f05b9251b"
      },
      "source": [
        "brown.sents()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsccW_3XkJDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bfe8fcae-4a1a-4721-bd13-52ded2ed4e4e"
      },
      "source": [
        "brown.sents(fileids='ca01')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHoOVcjDkPzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61113efb-3de4-488a-cfe4-0550b580026d"
      },
      "source": [
        "len(brown.fileids())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqhRiegUkYBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "82fa412a-6856-43d1-9bf7-da2a5c8c9c72"
      },
      "source": [
        "print(brown.fileids()[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds1fr-MekaF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f24f7b1f-cdcf-4442-f605-f1f32c4a5aaf"
      },
      "source": [
        "print(brown.raw('cb01').strip()[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
            "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
            "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
            "\n",
            "\n",
            "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
            "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbyf4hXLkejw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c57cff43-5164-4786-c68f-78d09ddef09c"
      },
      "source": [
        "nltk.download('webtext')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRAJFlIflF9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import webtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMSt8gjnlIBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "28416e2c-afb9-4534-db88-8871707362c8"
      },
      "source": [
        "webtext.fileids()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['firefox.txt',\n",
              " 'grail.txt',\n",
              " 'overheard.txt',\n",
              " 'pirates.txt',\n",
              " 'singles.txt',\n",
              " 'wine.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX-Ky_HjlJAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "2f3b7635-6fc3-4b3c-db37-ee913fff5979"
      },
      "source": [
        "for i, line in enumerate(webtext.raw('grail.txt').split('\\n')):\n",
        "  if i > 10:\n",
        "    break\n",
        "  print(str(i) + ':\\t' + line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tSCENE 1: [wind] [clop clop clop] \n",
            "1:\tKING ARTHUR: Whoa there!  [clop clop clop] \n",
            "2:\tSOLDIER #1: Halt!  Who goes there?\n",
            "3:\tARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
            "4:\tSOLDIER #1: Pull the other one!\n",
            "5:\tARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
            "6:\tSOLDIER #1: What?  Ridden on a horse?\n",
            "7:\tARTHUR: Yes!\n",
            "8:\tSOLDIER #1: You're using coconuts!\n",
            "9:\tARTHUR: What?\n",
            "10:\tSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr4vJmsolaNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6f0bf61b-f7a5-4f80-d3a8-5faedc11622c"
      },
      "source": [
        "grail_no5 = webtext.raw('grail.txt').split('\\n')[5]\n",
        "print(grail_no5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQsbXVl9mMCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7YWCGqKmVU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "63e8529f-26eb-4cb9-f87e-d63d9190422b"
      },
      "source": [
        "nltk.download('punkt')\n",
        "sent_tokenize(grail_no5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARTHUR: I am, ...  and this is my trusty servant Patsy.',\n",
              " 'We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.',\n",
              " 'I must speak with your lord and master.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8f6He6lmXDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7cb3c965-2e28-4f79-8f77-d2f4ba8c68da"
      },
      "source": [
        "for sent in sent_tokenize(grail_no5):\n",
        "  print(word_tokenize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ARTHUR', ':', 'I', 'am', ',', '...', 'and', 'this', 'is', 'my', 'trusty', 'servant', 'Patsy', '.']\n",
            "['We', 'have', 'ridden', 'the', 'length', 'and', 'breadth', 'of', 'the', 'land', 'in', 'search', 'of', 'knights', 'who', 'will', 'join', 'me', 'in', 'my', 'court', 'at', 'Camelot', '.']\n",
            "['I', 'must', 'speak', 'with', 'your', 'lord', 'and', 'master', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_69QUJPNnRo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3bffce97-14e8-4088-f3ad-ef49813d169e"
      },
      "source": [
        "for sent in sent_tokenize(grail_no5):\n",
        "  print([word.lower() for word in word_tokenize(sent)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arthur', ':', 'i', 'am', ',', '...', 'and', 'this', 'is', 'my', 'trusty', 'servant', 'patsy', '.']\n",
            "['we', 'have', 'ridden', 'the', 'length', 'and', 'breadth', 'of', 'the', 'land', 'in', 'search', 'of', 'knights', 'who', 'will', 'join', 'me', 'in', 'my', 'court', 'at', 'camelot', '.']\n",
            "['i', 'must', 'speak', 'with', 'your', 'lord', 'and', 'master', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG-ur-3unbkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "436acc48-981f-437f-ed4d-8295943ab685"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_en = stopwords.words('english')\n",
        "print(stopwords_en)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeWXDYyqnv7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "79a8f118-484d-46fe-d9e2-0cd4f7a8e2a7"
      },
      "source": [
        "# Treat the multiple sentences as one document (no need to sent_tokenize)\n",
        "# Tokenize and lowercase\n",
        "grail_no5_ascii_tokens = [str(item) for item in word_tokenize(grail_no5)]\n",
        "grail_no5_tokenized_lowered = list(map(str.lower, grail_no5_ascii_tokens))\n",
        "print(grail_no5_tokenized_lowered)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arthur', ':', 'i', 'am', ',', '...', 'and', 'this', 'is', 'my', 'trusty', 'servant', 'patsy', '.', 'we', 'have', 'ridden', 'the', 'length', 'and', 'breadth', 'of', 'the', 'land', 'in', 'search', 'of', 'knights', 'who', 'will', 'join', 'me', 'in', 'my', 'court', 'at', 'camelot', '.', 'i', 'must', 'speak', 'with', 'your', 'lord', 'and', 'master', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfIVAL5QoFuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb7d1fe7-8e44-4905-86ca-194592859a43"
      },
      "source": [
        "type(word_tokenize(grail_no5)[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2M4v1nRoqei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9b4fc535-47c8-4379-ef18-bdd70a3dbfdb"
      },
      "source": [
        "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
        "\n",
        "# List comprehension.\n",
        "print([word for word in grail_no5_tokenized_lowered if word not in stopwords_en])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arthur', ':', ',', '...', 'trusty', 'servant', 'patsy', '.', 'ridden', 'length', 'breadth', 'land', 'search', 'knights', 'join', 'court', 'camelot', '.', 'must', 'speak', 'lord', 'master', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REzzLDIgp3vh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdc3ae04-c4a8-416f-fc34-a89b3e4e3c9a"
      },
      "source": [
        "from string import punctuation\n",
        "# It's a string so we have to them into a set type\n",
        "print('From string.punctuation:', type(punctuation), punctuation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uul8QGg2p3nA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "46a2c9eb-798c-4aef-f535-07da2cd7e7b0"
      },
      "source": [
        "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
        "print(stopwords_en_withpunct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'be', 'an', 'so', \"shouldn't\", '$', 'll', 'our', 'theirs', '`', 'if', 'about', 'after', 'aren', 'y', 'too', 'what', \"she's\", 'any', 'd', 'on', 'into', \"haven't\", \"shan't\", 'that', 'at', 'how', 'hadn', 'having', 'this', 'to', \"you'll\", \"hadn't\", 'm', 'before', 'mightn', 'them', 'wouldn', 'whom', 'ain', 'now', '_', 'where', 'very', 't', 'ma', 'herself', 'from', 'by', \"you're\", 'because', 'you', 'she', 'such', 'it', 'above', 'just', \"mightn't\", 'between', 'itself', 'down', 'not', 'under', '-', \"mustn't\", '%', 'same', 'or', 'should', 'than', \"don't\", 'for', '~', 's', 'does', 'won', 're', '|', 'most', 'is', 'further', '<', 'wasn', 'shan', \"you'd\", 'he', '/', '#', 'some', 'then', 'who', 'here', 'his', 'own', '?', 'a', 'doesn', 'hers', 'when', \"isn't\", 'they', 'over', 'few', \"won't\", \"you've\", 'of', 'all', 'don', 'can', 'up', 'more', 'themselves', 'with', \"wouldn't\", '\"', '&', '@', \"weren't\", 'ours', '[', 'yours', 'have', 'and', 'during', 'only', 'o', 'are', 'while', 'other', 'no', 'will', 'nor', '+', 'himself', 'your', 'ourselves', 'were', 'these', \"that'll\", 'am', 'why', 'needn', \"aren't\", '(', '*', 'below', ')', '\\\\', 'didn', \"couldn't\", 'haven', ':', 'those', 'did', 'there', 'doing', \"wasn't\", \"it's\", 'couldn', '>', 'do', 'through', 'her', \"didn't\", 'isn', \"'\", '!', 'was', 'yourselves', 'which', 'i', '{', 'against', 'again', 'mustn', ';', 'their', 'had', 'the', 'has', 'being', 'as', 'off', 'out', ']', \"should've\", 'weren', 'hasn', 'but', 've', '=', 'myself', 'until', 'once', 'me', 'my', 'in', '}', 'been', 'its', '.', \"doesn't\", 'we', \"hasn't\", 'yourself', '^', ',', 'each', \"needn't\", 'both', 'him', 'shouldn'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyysvhF7p3ZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bd9d7922-7294-4249-a1bb-60c5c27be1e7"
      },
      "source": [
        "print([word for word in grail_no5_tokenized_lowered if word not in stopwords_en_withpunct])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arthur', '...', 'trusty', 'servant', 'patsy', 'ridden', 'length', 'breadth', 'land', 'search', 'knights', 'join', 'court', 'camelot', 'must', 'speak', 'lord', 'master']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIAgwStYqM7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0c8e1d19-f7e2-43dc-81bc-7a2375311ea0"
      },
      "source": [
        "# Stopwords from stopwords-json\n",
        "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
        "stopwords_json_en = set(stopwords_json['en'])\n",
        "stopwords_nltk_en = set(stopwords.words('english'))\n",
        "stopwords_punct = set(punctuation)\n",
        "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
        "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
        "\n",
        "# Remove the stopwords from `single_no8`.\n",
        "print('With combined stopwords:')\n",
        "print([word for word in grail_no5_tokenized_lowered if word not in stoplist_combined])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With combined stopwords:\n",
            "['arthur', '...', 'trusty', 'servant', 'patsy', 'ridden', 'length', 'breadth', 'land', 'search', 'knights', 'join', 'court', 'camelot', 'speak', 'lord', 'master']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siR_jeuvqa1T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncom9kk2qQ-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bd124eb7-90fe-4724-8acb-0ba8c7d3733e"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(porter.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "walk\n",
            "walk\n",
            "walk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrJC7oAqg59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c58d4999-4634-49ca-8051-9542407534a4"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(wnl.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "walking\n",
            "walk\n",
            "walked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqNiNEGAqg3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d4cab764-343f-4170-91c1-1f2fd1977ba3"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' # if mapping isn't found, fall back to Noun.\n",
        "    \n",
        "# `pos_tag` takes the tokenized sentence as input, i.e. list of string,\n",
        "# and returns a tuple of (word, tg), i.e. list of tuples of strings\n",
        "# so we need to get the tag from the 2nd element.\n",
        "\n",
        "walking_tagged = pos_tag(word_tokenize('He is walking to school'))\n",
        "print(walking_tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('He', 'PRP'), ('is', 'VBZ'), ('walking', 'VBG'), ('to', 'TO'), ('school', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGYV2BESqg0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb594807-0b1c-4d81-f990-6842e2ac8e90"
      },
      "source": [
        "[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in walking_tagged]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtag3FYrqgxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfe21bd0-0d7a-417d-c1d1-69a414bd6d62"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
        "                  'VB':'v', 'RB':'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n' \n",
        "    \n",
        "def lemmatize_sent(text): \n",
        "    # Text input is string, returns lowercased strings.\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
        "            for word, tag in pos_tag(word_tokenize(text))]\n",
        "\n",
        "lemmatize_sent('He is walking to school')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'be', 'walk', 'to', 'school']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DdBavdorIIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5880ad6d-63d6-4afb-809e-7f88eaa50e53"
      },
      "source": [
        "print('Original Grail no. 5:')\n",
        "print(grail_no5, '\\n')\n",
        "print('Lemmatized and removed stopwords:')\n",
        "print([word for word in lemmatize_sent(grail_no5) \n",
        "       if word not in stoplist_combined\n",
        "       and not word.isdigit() ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Grail no. 5:\n",
            "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master. \n",
            "\n",
            "Lemmatized and removed stopwords:\n",
            "['arthur', '...', 'trusty', 'servant', 'patsy', 'ride', 'length', 'breadth', 'land', 'search', 'knight', 'join', 'court', 'camelot', 'speak', 'lord', 'master']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDvuaNmArH7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Input: str, i.e. document/sentence\n",
        "    # Output: list(str) , i.e. list of lemmas\n",
        "    return [word for word in lemmatize_sent(text) \n",
        "            if word not in stoplist_combined\n",
        "            and not word.isdigit()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsiygp2wrX67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "# Lemmatize and remove stopwords\n",
        "processed_sent1 = preprocess_text(sent1)\n",
        "processed_sent2 = preprocess_text(sent2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWpVLpvMrXxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f2833e53-874a-4776-9c3e-855fbc66e1f0"
      },
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent1)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed sentence:\n",
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "\n",
            "Word counts:\n",
            "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvGJUMZsOVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6773db8d-76c9-4cca-c49a-1724880cedb8"
      },
      "source": [
        "print('Processed sentence:')\n",
        "print(processed_sent2)\n",
        "print()\n",
        "print('Word counts:')\n",
        "print(Counter(processed_sent2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed sentence:\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Word counts:\n",
            "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'fox': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cus_ufVsOR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = u\"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = u\"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Create the vectorizer\n",
        "    count_vect = CountVectorizer()\n",
        "    count_vect.fit_transform(fin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyDz-ljtsOOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "dc772bd1-e1d7-41cc-994b-2f1c20af8112"
      },
      "source": [
        "# We can check the vocabulary in our vectorizer\n",
        "# It's a dictionary where the words are the keys and \n",
        "# The values are the IDs given to each word. \n",
        "count_vect.vocabulary_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0,\n",
              " 'dog': 1,\n",
              " 'fox': 2,\n",
              " 'jumps': 3,\n",
              " 'lazy': 4,\n",
              " 'mr': 5,\n",
              " 'over': 6,\n",
              " 'quick': 7,\n",
              " 'the': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64JZi2NBsOLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "24214548-23d3-4cdd-b3bd-9926a9927ef4"
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = u\"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = u\"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
        "                                 tokenizer=word_tokenize)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0, 'dog': 1, 'fox': 2, 'jumps': 3, 'lazy': 4, 'mr': 5, 'quick': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlGdwsW_swjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8993328-077b-4cd2-feb5-6897e16bfd8d"
      },
      "source": [
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sent1 = u\"The quick brown fox jumps over the lazy brown dog.\"\n",
        "sent2 = u\"Mr brown jumps over the lazy fox.\"\n",
        "\n",
        "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
        "    # Override the analyzer totally with our preprocess text\n",
        "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
        "    count_vect.fit_transform(fin)\n",
        "count_vect.vocabulary_ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 0, 'dog': 1, 'fox': 2, 'jump': 3, 'lazy': 4, 'mr': 5, 'quick': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTOBURG7s5Nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0606cbc3-529a-434f-d006-9148564a46f6"
      },
      "source": [
        "count_vect.transform([sent1, sent2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 11 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3imf8kGps5Kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "52004f02-9453-43f1-ec88-98945ff49e06"
      },
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "# Print the words sorted by their index\n",
        "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
        "\n",
        "print(preprocess_text(sent1))\n",
        "print(preprocess_text(sent2))\n",
        "print()\n",
        "print('Vocab:', words_sorted_by_index)\n",
        "print()\n",
        "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
            "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
            "\n",
            "Vocab: ('brown', 'dog', 'fox', 'jump', 'lazy', 'mr', 'quick')\n",
            "\n",
            "Matrix/Vectors:\n",
            " [[2 1 1 1 1 0 1]\n",
            " [1 0 1 1 1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_pwnJurukj6",
        "colab_type": "text"
      },
      "source": [
        "## NLTK [Ubiquitous Ambiguity](https://www.nltk.org/book/ch08.html)\n",
        "A well-known example of ambiguity is shown in (2), from the Groucho Marx movie, Animal Crackers (1930):\n",
        "\n",
        "(2)\t\tWhile hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\n",
        "\n",
        "Let's take a closer look at the ambiguity in the phrase: I shot an elephant in my pajamas. First we need to define a simple grammar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m7EcPvJtkca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  PP -> P NP\n",
        "  NP -> Det N | Det N PP | 'I'\n",
        "  VP -> V NP | VP PP\n",
        "  Det -> 'an' | 'my'\n",
        "  N -> 'elephant' | 'pajamas'\n",
        "  V -> 'shot'\n",
        "  P -> 'in'\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaDcdo3gs5IR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "87743f0e-648e-41d2-d06a-10e1501e5bbc"
      },
      "source": [
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser = nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8wbXUKuLGX",
        "colab_type": "text"
      },
      "source": [
        "(a)\n",
        "![alt_text](https://www.nltk.org/book/tree_images/ch08-tree-1.png)\n",
        "---\n",
        "(b)\n",
        "![alt text](https://www.nltk.org/book/tree_images/ch08-tree-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Szc5ikeyJnQ",
        "colab_type": "text"
      },
      "source": [
        "## Install [SpaCy](https://nlpforhackers.io/complete-guide-to-spacy/) (en)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cOQ5YwCvY58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a17f7e11-acad-429f-acf5-2054da3084a1"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF9Vz85ps5FA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b3ca299d-53c3-47dd-878d-5b8b5d1b71de"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u'Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Hello\"\n",
            "\"    \"\n",
            "\"World\"\n",
            "\"!\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unrCeQSYvATg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "adc3cdbf-a1f5-4c1a-9ae0-7d3d239d7508"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u'Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"', token.idx)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Hello\" 0\n",
            "\"    \" 6\n",
            "\"World\" 10\n",
            "\"!\" 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5G3qDJvmv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "70f52107-ebd2-4f98-ba23-60f531d3cc8e"
      },
      "source": [
        "# Token class exposes a lot of word-level attributes\n",
        "\n",
        "doc = nlp(u\"Next week I'll   be in Madrid.\")\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next\t0\tnext\tFalse\tFalse\tXxxx\tADJ\tJJ\n",
            "week\t5\tweek\tFalse\tFalse\txxxx\tNOUN\tNN\n",
            "I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
            "'ll\t11\twill\tFalse\tFalse\t'xx\tVERB\tMD\n",
            "  \t15\t  \tFalse\tTrue\t  \tSPACE\t_SP\n",
            "be\t17\tbe\tFalse\tFalse\txx\tVERB\tVB\n",
            "in\t20\tin\tFalse\tFalse\txx\tADP\tIN\n",
            "Madrid\t23\tmadrid\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
            ".\t29\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heZQU1fEvnUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5788150c-a9a1-4c23-d3be-7d2f7d76ce20"
      },
      "source": [
        "# Sentence Detection\n",
        "\n",
        "doc = nlp(u\"These are apples. These are oranges.\")\n",
        " \n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "These are apples.\n",
            "These are oranges.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-jw7OScvsDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ba9c553b-ba77-4c0f-8533-1f5c6c995505"
      },
      "source": [
        "# POS Tagging\n",
        "\n",
        "doc = nlp(u\"Next week I'll be in Madrid.\")\n",
        "print([(token.text, token.tag_) for token in doc])\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('in', 'IN'), ('Madrid', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV23hKfpvsBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "51c3e964-0d89-4e3f-ea52-94c5f7d911f1"
      },
      "source": [
        "# NER Named Entity Recognition\n",
        "\n",
        "doc = nlp(u\"Next week I'll be in Madrid.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next week DATE\n",
            "Madrid GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jMvAiWtvr-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f09e7418-f774-4475-d705-459ab4b746d9"
      },
      "source": [
        "# Spacy Entity Types\n",
        "\n",
        "doc = nlp(u\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 CARDINAL\n",
            "9 a.m. TIME\n",
            "30% PERCENT\n",
            "just 2 days DATE\n",
            "WSJ ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5TG0AV7vr7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5866c2fe-6e80-4fe5-ba73-072ec1b7bdf9"
      },
      "source": [
        "# displaCy\n",
        "\n",
        "from spacy import displacy\n",
        " \n",
        "doc = nlp(u'I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">I just bought \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    2\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " shares at \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    9 a.m.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " because the stock went up \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    30%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    just 2 days\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " according to the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    WSJ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrN2aHz2vr4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c0a9a32c-cf82-4117-a510-29e1dda89687"
      },
      "source": [
        "# Chunking\n",
        "\n",
        "doc = nlp(u\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.label_, chunk.root.text)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall Street Journal NP Journal\n",
            "an interesting piece NP piece\n",
            "crypto currencies NP currencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DW-AFGiwm2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f476c8e-b52a-4fc1-993e-42582a3948c5"
      },
      "source": [
        "# Dependency Parsing\n",
        "\n",
        "doc = nlp(u'Wall Street Journal just published an interesting piece on crypto currencies')\n",
        " \n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall/NNP <--compound-- Street/NNP\n",
            "Street/NNP <--compound-- Journal/NNP\n",
            "Journal/NNP <--nsubj-- published/VBD\n",
            "just/RB <--advmod-- published/VBD\n",
            "published/VBD <--ROOT-- published/VBD\n",
            "an/DT <--det-- piece/NN\n",
            "interesting/JJ <--amod-- piece/NN\n",
            "piece/NN <--dobj-- published/VBD\n",
            "on/IN <--prep-- piece/NN\n",
            "crypto/JJ <--compound-- currencies/NNS\n",
            "currencies/NNS <--pobj-- on/IN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNnF0IUKwqmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "c2e2125a-c726-4141-a8f6-e15164f5f705"
      },
      "source": [
        "# Visualizing Dependency Parsing\n",
        "\n",
        "from spacy import displacy\n",
        " \n",
        "doc = nlp(u'Wall Street Journal just published an interesting piece on crypto currencies')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"76-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wall</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">Street</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Journal</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">just</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">published</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">an</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">interesting</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">piece</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">crypto</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">currencies</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-6\" stroke-width=\"2px\" d=\"M430,137.0 C430,2.0 680.0,2.0 680.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M680.0,139.0 L688.0,127.0 672.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-7\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-76-0-9\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-76-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945.0,139.0 L953.0,127.0 937.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiHaVsU_w0Am",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "e0c7ea7e-4a65-4ba5-b445-2b03c823d31f"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
            "\u001b[K    100% || 852.3MB 122.1MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-lg-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EnZKPCQwz8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the en_core_web_lg embeddings \n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvaC9K0kxkGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "1801a33b-602d-4b5f-d79c-e0b2e393b6fb"
      },
      "source": [
        "# View vector representation for the word 'banana'\n",
        "\n",
        "print(nlp.vocab[u'banana'].vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
            " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
            " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
            "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
            "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
            " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
            "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
            " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
            "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
            "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
            " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
            " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
            "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
            "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
            " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
            "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
            " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
            " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
            "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
            "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
            "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
            "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
            "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
            " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
            " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
            "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
            "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
            "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
            "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
            " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
            "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
            " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
            "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
            " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
            "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
            "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
            " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
            "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
            "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
            " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
            " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
            "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
            " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
            "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
            " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
            " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
            "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
            "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
            "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
            " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN0BORcNwz56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ef1390e-0b55-43d8-8d7e-df17d5ae589a"
      },
      "source": [
        "# Word embedding Math: \"queen\" = \"king\" \n",
        "\n",
        "from scipy import spatial\n",
        " \n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        " \n",
        "man = nlp.vocab[u'man'].vector\n",
        "woman = nlp.vocab[u'woman'].vector\n",
        "queen = nlp.vocab[u'queen'].vector\n",
        "king = nlp.vocab[u'king'].vector\n",
        " \n",
        "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
        "maybe_king = man - woman + queen\n",
        "computed_similarities = []\n",
        " \n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors\n",
        "    if not word.has_vector:\n",
        "        continue\n",
        " \n",
        "    similarity = cosine_similarity(maybe_king, word.vector)\n",
        "    computed_similarities.append((word, similarity))\n",
        " \n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "print([w[0].text for w in computed_similarities[:10]])\n",
        " \n",
        "# ['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'KINGS', 'kings', 'Kings']\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'Kings', 'KINGS', 'kings']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgzWmQW3wz3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7651494-99fa-47da-fc9c-5ec0f9f5457c"
      },
      "source": [
        "# Computing Similiarity\n",
        "\n",
        "banana = nlp.vocab[u'banana']\n",
        "dog = nlp.vocab[u'dog']\n",
        "fruit = nlp.vocab[u'fruit']\n",
        "animal = nlp.vocab[u'animal']\n",
        " \n",
        "print(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\n",
        "print(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.66185343 0.23552851\n",
            "0.67148364 0.24272855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRWOetg-wz0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "14d7478d-31bf-4fe2-a690-96f859303dd4"
      },
      "source": [
        "# Computing Similarity on entire texts\n",
        "\n",
        "target = nlp(u\"Cats are beautiful animals.\")\n",
        " \n",
        "doc1 = nlp(u\"Dogs are awesome.\")\n",
        "doc2 = nlp(u\"Some gorgeous creatures are felines.\")\n",
        "doc3 = nlp(u\"Dolphins are swimming mammals.\")\n",
        " \n",
        "print(target.similarity(doc1))  # 0.8901765218466683\n",
        "print(target.similarity(doc2))  # 0.9115828449161616\n",
        "print(target.similarity(doc3))  # 0.7822956752876101"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8901766262114666\n",
            "0.9115828449161616\n",
            "0.7822956256736615\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}